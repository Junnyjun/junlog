# Record

카프카에서 \*\*레코드(Record)\*\*는 메시지(Message) 또는 \*\*이벤트(Event)\*\*라고도 불리며, 프로듀서가 발행하고 컨슈머가 소비하는 데이터의 최소 단위입니다. 레코드는 단순한 데이터 덩어리가 아니라, 분산 시스템에서 효율성과 정확성을 보장하기 위한 특정 구조를 가지고 있습니다.

### 1. 레코드의 핵심 구성 요소

카프카의 레코드는 기본적으로 다음 네 가지 주요 요소로 구성됩니다.

| 구성 요소             | 설명                                                           | 역할                                                    |
| ----------------- | ------------------------------------------------------------ | ----------------------------------------------------- |
| 값 (Value)         | 레코드의 실제 데이터입니다. (예: 사용자 로그 내용, 주문 정보 JSON, 센서 측정값)           | 컨슈머가 필요로 하는 실질적인 정보를 담고 있습니다.                         |
| 키 (Key)           | 선택 사항이지만 매우 중요하며, 레코드의 식별자 역할을 합니다. (예: `UserID`, `OrderID`) | 순서 보장 및 데이터 분배(파티셔닝) 기준이 됩니다. 동일 키는 항상 동일 파티션에 저장됩니다. |
| 타임스탬프 (Timestamp) | 레코드가 생성된 시간 또는 브로커에 저장된 시간입니다.                               | 데이터의 시간 순서를 추적하고 시간 기반 보존 정책에 활용됩니다.                  |
| 헤더 (Headers)      | 선택 사항이며, 메타데이터를 담는 영역입니다. (예: 추적 ID, 애플리케이션 버전)              | 레코드의 내용과 직접 관련이 없지만, 라우팅이나 처리 로직에 필요한 추가 정보를 제공합니다.   |

### 2. 레코드의 불변성 (Immutability)

카프카 레코드의 가장 중요한 특성 중 하나는 불변성입니다.

* 한 번 기록되면 영원히: 레코드가 파티션에 기록되고 오프셋이 부여된 후에는 그 내용(키, 값, 타임스탬프 등)이나 순서를 수정할 수 없습니다.
* 로그 중심 아키텍처: 카프카가 \*\*분산 커밋 로그(Distributed Commit Log)\*\*를 기반으로 하기 때문입니다. 마치 회계 장부처럼, 모든 이벤트는 발생한 순서대로 기록되며, 수정은 오직 새로운 이벤트(수정 또는 삭제 이벤트)를 추가하는 방식으로만 이루어집니다.
* 단순성과 안정성: 불변성은 브로커의 역할을 단순화하고, 복제 과정에서 데이터의 정합성을 보장하며, 컨슈머가 안심하고 데이터를 재처리(Replay)할 수 있는 근거가 됩니다.

### 3. 직렬화와 역직렬화 (Serialization and Deserialization)

레코드가 네트워크를 통해 전송되거나 디스크에 저장될 때, 그리고 컨슈머가 이를 다시 읽어 처리할 때 직렬화/역직렬화 과정이 필수적으로 일어납니다.

* 직렬화 (Serialization): 프로듀서가 애플리케이션에서 사용하는 객체(예: Java Object, JSON 문자열)를 네트워크를 통해 전송 가능한 바이트 배열(byte array) 형태로 변환하는 과정입니다. 카프카 브로커는 이 바이트 배열 형태로 데이터를 저장합니다.
* 역직렬화 (Deserialization): 컨슈머가 브로커로부터 받은 바이트 배열을 다시 애플리케이션에서 처리 가능한 객체 형태로 변환하는 과정입니다.
* 스키마의 중요성: 직렬화/역직렬화 시 데이터의 형태를 정의하는 \*\*스키마(Schema)\*\*를 일관성 있게 관리하는 것이 중요합니다. Avro, Protobuf, JSON Schema 등이 널리 사용되며, 스키마 레지스트리를 통해 관리됩니다.

### 4. 레코드가 성능에 미치는 영향

레코드의 구조는 카프카의 고성능을 뒷받침하는 여러 기술과 연결됩니다.

* 압축 (Compression): 레코드를 브로커로 전송하기 전에 묶음(Batch) 단위로 압축할 수 있습니다. 카프카는 이 압축된 상태 그대로 레코드를 저장하고, 컨슈머가 가져갈 때까지 유지합니다. 이를 통해 네트워크 대역폭 사용량과 디스크 I/O를 크게 줄여 처리량을 높입니다.
* 배치 처리 (Batching): 프로듀서는 레코드를 개별적으로 전송하지 않고, 메모리에 일정 크기 또는 시간만큼 레코드를 모아서(Batching) 한 번에 브로커로 전송합니다. 이는 \*\*네트워크 왕복 횟수(RTT)\*\*를 최소화하여 처리량을 극대화합니다.
* 키와 파티셔닝: 레코드의 키는 데이터의 분산 방식을 결정합니다. 키를 적절히 사용하여 데이터를 파티션에 균등하게 분산시키는 것이 브로커 간의 부하 균형을 맞추고 시스템 전체의 성능을 유지하는 핵심입니다.
